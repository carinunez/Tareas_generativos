{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "gkGLkfuPR11L"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carinunez/Tareas_generativos/blob/main/Tarea_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tarea 3: Autoencoderes Variacionales\n",
        "\n",
        "### MDS7203 Modelos Generativos Profundos\n",
        "\n",
        "**Nombre:**\n",
        "Carolina Nuñez y Hecmar Taucare\n",
        "\n",
        "**Fecha de entrega:**\n",
        "\n",
        "En esta tercera tarea se implementará un VAE para el dataset de juguete [`3d-shapes`](https://github.com/google-deepmind/3d-shapes). Los objetivos serán implementar una red neuronal acorde al dataset de entrenamiento, entrenar el VAE y estudiar las propiedades semánticas del espacio latente inducido.\n",
        "\n",
        "Algunas instrucciones generales:\n",
        "\n",
        "- Se pueden utilizar de manera libre herramientas como ChatGPT y Claude, entre otras.\n",
        "- Para la entrega, no es necesario un informe, este archivo es suficiente.\n",
        "- Se debe entregar el documento con todas las celdas ejecutadas.\n",
        "- La tarea debe ser realizada en Google Colab."
      ],
      "metadata": {
        "id": "V9Jfl4wVLW-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "EjwDEwu6DKtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datos de entrenamiento\n",
        "\n",
        "El dataset `3d-shapes` es un dataset artificial construido a partir de 6 factores latentes claramente definidos ([ver ejemplo](https://github.com/google-deepmind/3d-shapes/blob/master/3dshapes_loading_example.ipynb)):\n",
        "\n",
        "```\n",
        "_FACTORS_IN_ORDER = ['floor_hue', 'wall_hue', 'object_hue', 'scale', 'shape', 'orientation']\n",
        "```\n",
        "\n",
        "Para la construcción de este dataset se consideraron distintos valores para cada factor latente. La cantidad de valores distintos para cada factor viene dada en el siguiente diccionario:\n",
        "\n",
        "```\n",
        "_NUM_VALUES_PER_FACTOR = {\n",
        "    'floor_hue':   10,\n",
        "    'wall_hue':    10,\n",
        "    'object_hue':  10,\n",
        "    'scale':       8,\n",
        "    'shape':       4,\n",
        "    'orientation': 15\n",
        "    }\n",
        "```\n",
        "\n",
        "Lo que da un total de $10\\times10\\times10\\times8\\times4\\times15=480\\,000$ imágenes.\n",
        "\n",
        "- ¿Qué define cada uno de estos factores latentes? ¿Cuáles son los distintos valores o rangos que pueden tomar estos factores latentes?\n",
        "> **Respuesta:**\n",
        "floor_hue representa el color del piso  y toma 10 valores separados linealmente entre 0 y 1\n",
        "wall_hue corresponde al color de la pared, object_hue=color del objeto (ambos toman 10 valores), scale es el tamaño del objeto con 8 valores entre 0 y 1, shape es la forma del objeto tomando valores discretos enteros entre {0,1,2,3} y orientation (es el ángulo de rotación del objeto) tomando 15 valores equidistantes en el rango entre [-30°.30°]\n",
        "- Considerando la naturaleza de cada factor latente, ¿qué distribuciones de probabilidad se podrían asociar a cada factor?\n",
        "> **Respuesta:**\n",
        "floor_hue toma valores equiespaciados entre 0 y 1, por lo que se podría asociar una distribución uniforme discreta sobre 10 valores, al igual que wall_he y object_hue. Mientras que scale sería una uniforme discreta de 8 valores (entre 0 y 1), por otro lado shape sería una distribución categórica uniforme tomando los valores {0,1,2,3} y orientation sería una uniforme discreta sobre 15 valores.\n",
        "Las imágenes asociadas a este dataset están contenidas en el [siguiente archivo](https://console.cloud.google.com/storage/browser/3d-shapes;tab=objects?inv=1&invt=Abx13w&prefix=&forceOnObjectsSortingFiltering=false):"
      ],
      "metadata": {
        "id": "Zehf95dmMRLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil -m cp gs://3d-shapes/3dshapes.h5 ."
      ],
      "metadata": {
        "id": "bT21pvsN-C90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = h5py.File('3dshapes.h5', 'r')\n",
        "print(\"Claves del archivo:\", list(f.keys()))\n",
        "\n",
        "# Forma de las imágenes,etiquetas y formato\n",
        "print(\"Tamaño de las imágenes:\", f['images'].shape)\n",
        "print(\"Tamaño de los labels:  \", f['labels'].shape)\n",
        "print(type(f['labels']))"
      ],
      "metadata": {
        "id": "DiWgdEjiZI42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3hvHTfICZ8BD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clase para el dataset\n",
        "\n",
        "- Implemente la clase `Shapes3D` utilizando el archivo descargado. Si tiene problemas de memoria RAM, puede fijar un parámetro `max_samples` para definir una cantidad máxima de muestras. En este caso, las imágenes seleccionadas para constituir el dataset deben ser elegidas de forma aleatoria sobre las $480\\,000$ imágenes que contiene el dataset original."
      ],
      "metadata": {
        "id": "7qAez-VcXUVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Shapes3D(Dataset):\n",
        "\n",
        "    def __init__(self, filename, max_samples=None, seed=0):\n",
        "        # nombre del archivo guardado\n",
        "        self.filename = filename\n",
        "        # se abre el archivo en el formato HDF5 para leer 'r'\n",
        "        self.file = h5py.File(self.filename, 'r')\n",
        "\n",
        "        # se guardan las imágenes y etiquetas\n",
        "        self._images = self.file['images']\n",
        "        self._labels = self.file['labels']\n",
        "\n",
        "        # cantidad de imágenes en total\n",
        "        self.total_samples = len(self._images)\n",
        "\n",
        "        # índices se sacan de forma aleatoria para tomar muestra\n",
        "        # replace = False es que sean distintos\n",
        "        if max_samples is not None:\n",
        "            rng = np.random.default_rng(seed)  # generador random con semilla\n",
        "            self.indices = rng.choice(self.total_samples, size=max_samples, replace=False)\n",
        "        else:\n",
        "            self.indices = np.arange(self.total_samples)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)  # cantidad de índices guardados\n",
        "\n",
        "    def __getitem__(self, n):\n",
        "        i = self.indices[n]  # se busca el índice\n",
        "\n",
        "        # capturamos la imagen y su etiqueta asociada\n",
        "        img = self._images[i].astype(np.float32) / 255.0  # normalización\n",
        "        img = np.transpose(img, (2, 0, 1))  # cambia orden de ejes: alto, ancho, canales → canales, alto, ancho\n",
        "        label = self._labels[i].astype(np.float32)\n",
        "        return torch.tensor(img), torch.tensor(label)"
      ],
      "metadata": {
        "id": "6V90opOmMp42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con la clase `Shapes3D` implementada, asumiendo que su constructor `__init__` recibe parámetros `filename` (nombre del archivo descargado) y `max_samples` (cantidad de muestras en el dataset), se puede instanciar el dataset y el respectivo dataloader.\n",
        "\n",
        "- Defina valores para `batch_size` y `max_samples` que sean compatible con el hardware disponible."
      ],
      "metadata": {
        "id": "9ZryJBLadiy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset = Shapes3D(filename='3dshapes.h5', max_samples=32)"
      ],
      "metadata": {
        "id": "2wsPbZODNEPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "max_samples = 32\n",
        "\n",
        "dataset = Shapes3D(filename='3dshapes.h5', max_samples=max_samples)\n",
        "dataloader = DataLoader(dataset, batch_size, shuffle=True, num_workers=2, pin_memory=True)"
      ],
      "metadata": {
        "id": "C9znCEyQFalW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ver el primer batch\n",
        "for images, labels in dataloader:\n",
        "    print(\"Tamaño del batch de imágenes:\", images.shape)\n",
        "    print(\"Tamaño del batch de labels:\", labels.shape)\n",
        "    break  # solo ver el primero"
      ],
      "metadata": {
        "id": "AaSrTm76NIUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_idx, (images, labels) in enumerate(dataloader):\n",
        "    print(f\"⭐ Batch {batch_idx + 1}\")\n",
        "    print(\"  Tamaño del batch de imágenes:\", images.shape)\n",
        "    print(\"  Tamaño del batch de labels:  \", labels.shape)"
      ],
      "metadata": {
        "id": "wGkVsCgcNYq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ¿Qué técnicas de data augmentation se podrían haber usado en este dataset? ¿Qué técnicas serían contraproducentes para el entrenamiento?\n",
        "> **Respuesta:**\n",
        "recortes de ciertas partes de la imagen aleatorios, desordenar la imagen para que el modelo aprendiera más texturas aunque sean figuras algo simples, dejar algunas partes en negro, agregar algo de ruido gaussiano.\n",
        "\n",
        "Las que no servirian serian cambios de color, rotacion, escala, es decir todos los factores latentes que se buscan capturar con el modelo, ya que, estos existen y las imagenes creadas son una combinación de estos factores, por lo que esas transformaciones darían sobreajuste, ya que, sería aprender sobre lo mismo, y como algunos son factores latentes estan distribuidos linealmente sería aún más fácil que aprenda el modelo"
      ],
      "metadata": {
        "id": "1kzjRyAThYRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualización\n",
        "\n",
        "La siguiente función auxiliar se usará para visualizar un batch de imágenes genérico:"
      ],
      "metadata": {
        "id": "0Bao4bY2V8LA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_images(imgs, ncols=None):\n",
        "    imgs = imgs.permute(0, 2, 3, 1).detach().cpu()\n",
        "    n_images = imgs.shape[0]\n",
        "    ncols = ncols or int(n_images**0.5)\n",
        "    nrows = -(-n_images // ncols)\n",
        "\n",
        "    fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 2, nrows * 2))\n",
        "    for ax, img in zip(axes.flat, imgs):\n",
        "        ax.imshow(img)\n",
        "        ax.axis('off')\n",
        "    for ax in axes.flat[n_images:]:\n",
        "        ax.set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "97XUk1E7Db5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgs, labels = next(iter(dataloader))\n",
        "assert imgs.shape == (batch_size, 3, 64, 64)\n",
        "assert labels.shape == (batch_size, 6)\n",
        "\n",
        "show_images(imgs[:12], ncols=6)"
      ],
      "metadata": {
        "id": "z2glkuEuDxLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Red neuronal y entrenamiento\n",
        "\n",
        "### Clase `VAE`\n",
        "\n",
        "- Implemente la siguiente clase asociada a la red neuronal de un **VAE incondicional**. El método `forward` debe retornar el par $(\\mu_\\theta,\\sigma_\\theta)\\in\\mathbb{R}^L\\times\\mathbb{R}^L_{++}$ asociado al encoder y el tensor $r_\\phi\\in[0,1]^D$ asociado al decoder (revisar [implementación vista en clases](https://github.com/fernando-fetis/MDS7203/tree/main/Clases/Clase%2013/notebooks)). La red neuronal debe ser una red convolucional acorde a la complejidad del dataset, y puede usar cualquiera de las técnicas vistas durante el curso."
      ],
      "metadata": {
        "id": "bK5lMJfDVnSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nos basamos en una arquitectura similar a la vista en clases, utilizando\n",
        "# Conv2d porque estamos usando imag 2d de 3 canales, en lugar de datos como\n",
        "# en el ejemplo de clases\n",
        "\n",
        "class encoder_class(nn.Module):\n",
        "  def __init__(self, latent_dim):\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    # Arquitectura: 3 canales de 64x64 -> 32 canales de 32x32 ->\n",
        "    #               64 canales de 16x16 -> 128 canales de 8x8\n",
        "    self.flatten = nn.Flatten()\n",
        "    # self.fc = nn.Sequential(\n",
        "    #               nn.Linear(128 * 8 * 8, 256),\n",
        "    #               nn.ReLU())\n",
        "\n",
        "    self.fc_mu = nn.Linear(128*8*8, latent_dim)\n",
        "    self.fc_logvar = nn.Linear(128*8*8, latent_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = x.view(x.size(0), -1) # (N,256)\n",
        "    # x = self.fc(x)\n",
        "    mu = self.fc_mu(x)\n",
        "    logvar = self.fc_logvar(x)\n",
        "    return mu, torch.exp(0.5 * logvar) # std = sqrt(var) -> std =  exp(0.5*log(var))"
      ],
      "metadata": {
        "id": "glPFb_ImEmlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Probar que el encoder está funcionando\n",
        "latent_dim = 8\n",
        "encoders = encoder_class(latent_dim)\n",
        "x = torch.randn(batch_size, 3, 64, 64)\n",
        "mean, std = encoders(x)  # q(z|x).\n",
        "\n",
        "assert mean.shape == (batch_size, latent_dim)\n",
        "assert std.shape == (batch_size, latent_dim)"
      ],
      "metadata": {
        "id": "PIUAGxQmIBjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class decoder_class(nn.Module):\n",
        "  def __init__(self, latent_dim):\n",
        "    super().__init__()\n",
        "\n",
        "    # Proyecta el vector latente a  un tensor de 128x8x8 para usarlo como entrada\n",
        "    # de las capas conv\n",
        "    self.fc = nn.Sequential (\n",
        "        nn.Linear(latent_dim, 128*8*8),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    self.dec = nn.Sequential(\n",
        "        nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1),\n",
        "        # Utilizamos Sigmoid para obtener valores entre 0 y 1, ya que las\n",
        "        # imágenes están normalizadas (ver arriba)\n",
        "        nn.Sigmoid(),\n",
        "    )\n",
        "\n",
        "  def forward(self, z):\n",
        "    x = self.fc(z)\n",
        "    x = x.view(z.size(0), 128, 8, 8)\n",
        "    return self.dec(x)"
      ],
      "metadata": {
        "id": "lOiHhQ13N4Hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = decoder_class(latent_dim)\n",
        "z = torch.randn(batch_size, latent_dim)\n",
        "mean = decoder(z)\n",
        "\n",
        "assert mean.shape == (batch_size, 3, 64, 64)"
      ],
      "metadata": {
        "id": "SwFdADbsOq8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "\n",
        "    def __init__(self, latent_dim):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        self.encoder = encoder_class(latent_dim)\n",
        "        self.decoder = decoder_class(latent_dim)\n",
        "        self.encoder_optimizer = optim.AdamW(self.encoder.parameters())\n",
        "        self.decoder_optimizer = optim.AdamW(self.decoder.parameters())\n",
        "\n",
        "      # reparametrizamos z para que sea calculable\n",
        "    def reparameterize(self, mu, std):\n",
        "        ep_noise = torch.randn_like(std)\n",
        "        return mu + ep_noise * std # Agregamos ruido a la Gaussiana\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, std = self.encoder(x)\n",
        "        z_noise = self.reparameterize(mu,std)\n",
        "        x_dec = self.decoder(z_noise)\n",
        "        return (mu, std), x_dec"
      ],
      "metadata": {
        "id": "2hTdeI0TEmvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consideraciones:\n",
        "En el notebook de la clase de usó MLP y transformaciones lineales, sin embargo como estaremos usando imágenes de figuras 3d, lo mejor es usar CNN porque:\n",
        "\n",
        "- Invarianza translacional: Detecta características independientemente de su posición\n",
        "- Compartición de parámetros: Muchísimos menos parámetros\n",
        "- Jerarquía de características: Bordes → texturas → formas → objetos\n",
        "- Preservación espacial: Mantiene relaciones geométricas cruciales para figuras 3D\n",
        "\n"
      ],
      "metadata": {
        "id": "T-cylEKS9bB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loop de entrenamiento\n",
        "\n",
        "Como se vio en clases, la ELBO de un VAE está compuesta por un término de reconstrucción y un término regularizador llamado prior matching:\n",
        "\n",
        "\\begin{equation}\n",
        "\\operatorname{ELBO}(x):=\\mathbb{E}_{q_\\phi(z|x)}\\left[\\log p_\\theta(x|z)\\right] - \\operatorname{{D}_{KL}}\\left(q_\\phi(z|x)\\| p_\\theta(z)\\right)\n",
        "\\end{equation}\n",
        "\n",
        "- ¿Qué diferencia esta función objetivo de la función objetivo usada en un autoencoder clásico? Dé una interpretación para ambos términos de la ELBO e indique cómo influyen en el entrenamiento de un VAE.\n",
        "> **Respuesta:**\n",
        "\n",
        "- La forma final del término de reconstrucción $\\mathbb{E}_{q_\\phi(z|x)}\\left[\\log p_\\theta(x|z)\\right]$ depende de la distribución $p_\\theta(x|z)$ elegida para el decoder. Indique qué distribución se asume implícitamente si el término de reconstrucción se calcula\n",
        "  - Usando MSE (i.e., como la distancia $L^1$ entre la reconstrucción y la muestra original).\n",
        "  > **Respuesta:**\n",
        "  - Usando BCE (i.e., usando la entropía cruzada binaria entre la reconstrucción y la muestra original).\n",
        "  > **Respuesta:**\n",
        "  - Usando la distancia $L^1$ entre la reconstrucción y la muestra original.\n",
        "  > **Respuesta:**\n",
        "\n",
        "- Complete la función `train_vae` que permite entrenar un VAE sobre un dataset de imágenes. Esta función debe contener las siguientes características:\n",
        "  - Permitir usar distintas métricas para el cálculo del término de reconstrucción. Esto se indica en el parámetro `reconstruction_metric`.\n",
        "  - Permitir disentangled representations usando la técnica propuesta en $\\beta$-VAE ([Higgins et al., 2017](https://openreview.net/forum?id=Sy2fzU9gl)). Esto se indica en el parámetro `beta`."
      ],
      "metadata": {
        "id": "gvbkgeQVWCTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_vae(model, dataloader, optimizer, reconstruction_metric, beta, n_epochs):\n",
        "\n",
        "    model = model.to(DEVICE)\n",
        "    model.train()\n",
        "\n",
        "    total_steps = n_epochs * len(dataloader)\n",
        "    global_bar = tqdm(total=total_steps, desc='Entrenamiento', position=0)\n",
        "\n",
        "    reconstruction_log, prior_matching_log = [], []\n",
        "\n",
        "    try:\n",
        "        for epoch in range(1, n_epochs + 1):\n",
        "            total_reconstruction, total_prior_matching = 0, 0\n",
        "\n",
        "            for x, _ in dataloader:\n",
        "                x = x.to(DEVICE)\n",
        "\n",
        "                # Entrenamiento -----------------------------------------------\n",
        "\n",
        "                # Distintas métricas de recontrucción\n",
        "                # Agregar parámetro beta -> beta-VAE\n",
        "\n",
        "                # ELBO:\n",
        "\n",
        "                encoder_mean, encoder_std = model.encoder(x)\n",
        "\n",
        "                prior_matching = 1/2 * (encoder_mean.norm(dim=-1) ** 2 +\n",
        "                                        encoder_std.norm(dim=-1) ** 2) - encoder_std.log().sum(dim=-1)\n",
        "                prior_matching = prior_matching.mean()\n",
        "\n",
        "                #z = torch.normal(encoder_mean, encoder_std)\n",
        "                z = encoder_mean + encoder_std * torch.randn_like(encoder_mean)\n",
        "                decoder_mean = model.decoder(z)\n",
        "\n",
        "\n",
        "                # reconstruction_term = - 1 / (2 * self.decoder_std ** 2) *\n",
        "                #  (x - decoder_mean).norm(dim=-1) ** 2\n",
        "\n",
        "                # Ahora la reconstruccion tomará la métrica señalada\n",
        "                reconstruction = reconstruction_metric(decoder_mean, x).sum(dim=-1)\n",
        "                reconstruction = reconstruction.mean()\n",
        "\n",
        "\n",
        "                elbo = reconstruction - beta * prior_matching\n",
        "                loss = - elbo.mean()\n",
        "\n",
        "                model.encoder_optimizer.zero_grad()\n",
        "                model.decoder_optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                model.encoder_optimizer.step()\n",
        "                model.decoder_optimizer.step()\n",
        "\n",
        "                # Registro -----------------------------------------------------\n",
        "\n",
        "                total_reconstruction += reconstruction.item()\n",
        "                total_prior_matching += prior_matching.item()\n",
        "\n",
        "                global_bar.update(1)\n",
        "                global_bar.set_postfix({\n",
        "                    'época': epoch,\n",
        "                    'reconstrucción': total_reconstruction / len(dataloader.dataset),\n",
        "                    'prior matching': total_prior_matching / len(dataloader.dataset)\n",
        "                })\n",
        "\n",
        "            N = len(dataloader.dataset)\n",
        "            reconstruction_log.append(total_reconstruction / N)\n",
        "            prior_matching_log.append(total_prior_matching / N)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print('Entrenamiento interrumpido.')\n",
        "\n",
        "    finally:\n",
        "        global_bar.close()\n",
        "\n",
        "        # Gráfico --------------------------------------------------------------\n",
        "\n",
        "        fig, ax1 = plt.subplots(figsize=(10, 4))\n",
        "\n",
        "        ax1.set_xlabel('Época')\n",
        "        ax1.set_ylabel('', color='red')\n",
        "        l1, = ax1.plot(reconstruction_log, label='Término de reconstrucción', color='red')\n",
        "        ax1.tick_params(axis='y', labelcolor='red')\n",
        "        ax1.set_ylabel('Escala término de reconstrucción')\n",
        "        # ax1.set_ylim(0.1, 0.6)\n",
        "\n",
        "        ax2 = ax1.twinx()\n",
        "        ax2.set_ylabel('', color='blue')\n",
        "        l2, = ax2.plot(prior_matching_log, label='Prior matching', color='blue')\n",
        "        ax2.tick_params(axis='y', labelcolor='blue')\n",
        "        # ax2.set_ylim(, 0.6)\n",
        "        ax2.set_ylabel('Escala prior matching')\n",
        "\n",
        "\n",
        "        lines = [l1, l2]\n",
        "        labels = [line.get_label() for line in lines]\n",
        "        ax1.legend(lines, labels)\n",
        "\n",
        "        plt.title('Dinámica de entrenamiento')\n",
        "        fig.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "vCRYss4dEgyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con todo lo anterior bien definido, se puede entrenar el VAE sobre el dataset `Shapes3D`.\n",
        "\n",
        "- Asigne valores a los hiperparámetros necesarios y entrene el modelo. Justifique su elección para los valores de `latent_dim`, `reconstruction_metric` y `beta`.\n",
        "\n",
        "> **Respuesta:**\n",
        "   Para asegurar la independencia de $q_ϕ$, se necesita un buen balance entre la preservación de la info y la capacidad de reconstrucción con $\\beta>1$\n"
      ],
      "metadata": {
        "id": "IzI73KDsLlHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 32\n",
        "reconstruction_metric = nn.BCELoss()\n",
        "beta = 0.001\n",
        "n_epochs = 10\n",
        "\n",
        "model = VAE(latent_dim)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, betas=(0.9, 0.999))\n",
        "\n",
        "train_vae(model, dataloader, optimizer, reconstruction_metric, beta, n_epochs)"
      ],
      "metadata": {
        "id": "ChZEQK9yErqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.parameters()"
      ],
      "metadata": {
        "id": "azAzxUSH3b_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tareas de generación\n",
        "\n",
        "En esta última parte se pide implementar algunas funciones simples para evaluar el entrenamiento del VAE.\n",
        "\n",
        "### Reconstrucción\n",
        "\n",
        "- Implemente la función `reconstruction` que retorne la reconstrucción que realiza el VAE sobre una imagen."
      ],
      "metadata": {
        "id": "psWHofJRwXC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reconstruction(x, model):\n",
        "  x = x.to(DEVICE)\n",
        "  model = model.to(DEVICE)\n",
        "  z_mean, z_std = model.encoder(x)\n",
        "  z = z_mean# + z_std * torch.randn_like(z_mean)\n",
        "  x_mean = model.decoder(z)\n",
        "  x_dec = x_mean# + self.decoder_std * torch.randn_like(x_mean)\n",
        "  return x_dec\n"
      ],
      "metadata": {
        "id": "r0fNw-mEQiDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgs, _ = next(iter(dataloader))\n",
        "\n",
        "x_original = imgs[:6].to(DEVICE)\n",
        "x_reconstruction = reconstruction(x_original, model)\n",
        "\n",
        "show_images(torch.cat([x_original, x_reconstruction], dim=0), ncols=len(x_original))"
      ],
      "metadata": {
        "id": "rhwKWu7IFAae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generación de nuevas muestras\n",
        "\n",
        "\n",
        "- Implemente la función `generate_samples` que genere muestras nuevas a partir del VAE."
      ],
      "metadata": {
        "id": "OI7moJOAwxCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_samples(model, n_samples):\n",
        "    ..."
      ],
      "metadata": {
        "id": "_hzUayxawx3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = generate_samples(model, n_samples=16)\n",
        "show_images(samples)"
      ],
      "metadata": {
        "id": "kRv-qy6rxzYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpolación\n",
        "\n",
        "El objetivo de esta pregunta es ver la diferencia entre una interpolación en el espacio de pixeles y el espacio latente inducido por el VAE.\n",
        "\n",
        "#### Interpolación en el espacio de pixeles\n",
        "\n",
        "- Implemente la función `interpolate_pixels` que interpole linealmente 2 imágenes. Notar que esta función no hace uso del VAE."
      ],
      "metadata": {
        "id": "-y-20fUIwcw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interpolate_pixels(x1, x2, steps=8):\n",
        "    ..."
      ],
      "metadata": {
        "id": "KmiN1CUxwslb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgs, _ = next(iter(dataloader))\n",
        "\n",
        "x1, x2 = imgs[0:2]\n",
        "pixel_interpolation = interpolate_pixels(x1, x2, steps=8)\n",
        "\n",
        "show_images(pixel_interpolation, ncols=len(pixel_interpolation))"
      ],
      "metadata": {
        "id": "-2W_8RxSyH84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Interpolación en el espacio latente\n",
        "\n",
        "- Implemente la función `interpolate_latents` que interpole 2 imágenes en el espacio latente del VAE entrenado."
      ],
      "metadata": {
        "id": "q0RBUA7Vwmi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interpolate_latents(x1, x2, model, steps=8):\n",
        "    ..."
      ],
      "metadata": {
        "id": "wV8PPXb2wk6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgs, _ = next(iter(dataloader))\n",
        "\n",
        "# Se interpolan las mismas imágenes usadas en interpolate_pixels:\n",
        "pixel_interpolation = interpolate_latents(x1, x2, model)\n",
        "\n",
        "show_images(pixel_interpolation, ncols=len(pixel_interpolation))"
      ],
      "metadata": {
        "id": "9XoYrmvEE0D-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aritmética en el espacio latente\n",
        "\n",
        "El objetivo de esta pregunta es realizar aritmética de atributos, donde los atributos, en este caso, vienen dados en la lista `_FACTORS_IN_ORDER` definida al comienzo.\n",
        "\n",
        "Recordar que, para un atributo dado, se puede calcular un centroide de dicho atributo, $z_{\\text{atributo}}\\in\\mathbb{R}^L$, promediando representaciones latentes de muestras que posean dicho atributo. Con esto, restando los centroides de dos atributos diferentes se obtiene un vector latente director, $z_{\\text{director}}\\in\\mathbb{R}^L$, el cual puede usarse para modificar atributos de una muestra $x\\in\\mathbb{R}^D$ mediante $x_\\lambda = \\text{decoder}(\\text{encoder}(x)+\\lambda z_\\text{director})$, donde $\\lambda>0$ es un parámetro de traslación.\n",
        "\n",
        "- Implemente la función `attribute_arithmetic`, la cual recibe 2 diccionarios (`source_attributes` y `target_attributes`) con los atributos que se usarán para definir el vector director. Este vector debe ser aplicado a una imagen dada (`test_img`) usando los ponderadores definidos en la lista `ponderators`."
      ],
      "metadata": {
        "id": "Q1Spsr1fzEnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attribute_arithmetic(model, dataset, source_attributes, target_attributes, test_img, ponderators):\n",
        "    ..."
      ],
      "metadata": {
        "id": "3CAQGg6D3BR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imagen de prueba:\n",
        "test_img, test_label = dataset[2]\n",
        "ponderators = torch.linspace(0, 1, 8).tolist()\n",
        "\n",
        "# Transformar forma original (cubo) -> esfera:\n",
        "source_attributes = {'shape': test_label[4]}\n",
        "target_attributes = {'shape': 2}\n",
        "\n",
        "displaced_img = attribute_arithmetic(model, dataset, source_attributes, target_attributes, test_img, ponderators)\n",
        "\n",
        "show_images(displaced_img, ncols=len(displaced_img))"
      ],
      "metadata": {
        "id": "E0SHL2rE-6Ti"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}